## üß† Boas Pr√°ticas para Projeto de Web Scraping/Crawler em **Java ou Kotlin** com Spring Boot + Selenium
> _Nota: este projeto ser√° desenvolvido em Kotlin, mas as diretrizes s√£o v√°lidas tamb√©m para Java._

Este documento define as diretrizes de qualidade para todo o c√≥digo gerado ou mantido no projeto. O objetivo √© garantir que o sistema seja limpo, modular, seguro, test√°vel e extens√≠vel. Nenhum exemplo pr√°tico deve ser inclu√≠do diretamente neste documento.

---

### ‚úÖ 1. Qualidade de C√≥digo (Clean Code)

- Nomes de vari√°veis, m√©todos e classes devem ser claros, descritivos e consistentes.
- M√©todos/fun√ß√µes devem ser curtos e com responsabilidade √∫nica.
- Eliminar repeti√ß√µes de l√≥gica (princ√≠pio DRY).
- Preferir composi√ß√£o a heran√ßa.
- Usar early return para reduzir aninhamento de c√≥digo.
- Evitar uso de `null`; utilizar `Optional` (Java) ou `nullable` com `safe calls` e `elvis` (Kotlin).
- N√£o utilizar `System.out.println`; utilizar loggers.

---

### ‚úÖ 2. Boas Pr√°ticas com Spring

- Usar inje√ß√£o de depend√™ncia por construtor.
- Utilizar corretamente as anota√ß√µes: `@Service`, `@Repository`, `@Component`, `@Configuration`.
- Utilizar `@Value` para propriedades externas.
- Organizar o projeto em pacotes/m√≥dulos com responsabilidade clara.

---

### ‚úÖ 3. Seguran√ßa e Robustez

- Validar todas as entradas externas.
- Implementar fallback de seletores.
- Evitar valores hardcoded.
- Tratar exce√ß√µes espec√≠ficas com logs detalhados.

---

### ‚úÖ 4. Scraping Inteligente

- Usar `WebDriverWait` para sincroniza√ß√£o de elementos.
- Alternar `User-Agent` dinamicamente.
- Aplicar delays aleat√≥rios.
- Persistir cookies para sess√µes futuras.
- Utilizar arquivos externos para seletores com fallback.

---

### ‚úÖ 5. Retry e Toler√¢ncia a Falhas

- Toda opera√ß√£o cr√≠tica deve utilizar retry.
- Aplicar `exponential backoff` em falhas.
- Registrar detalhes de cada tentativa.
- Utilizar `RetryHandler`, `RetryTemplate` ou mecanismo equivalente.

---

### ‚úÖ 6. Logging

- Utilizar SLF4J com `LoggerFactory`.
- Incluir nos logs: timestamp, correlation-id, URL, tipo de opera√ß√£o.
- Salvar logs em arquivos por data.

---

### ‚úÖ 7. Desacoplamento e Extensibilidade

- N√£o acoplar seletores e URLs no c√≥digo.
- Utilizar arquivos externos ou configura√ß√£o.
- Arquitetura deve permitir novos scrapers e estrat√©gias.

---

### ‚úÖ 8. Persist√™ncia e Valida√ß√£o

- Validar os dados antes de persistir.
- Permitir diferentes formas de persist√™ncia.
- Separar l√≥gica de scraping da persist√™ncia.

---

### ‚úÖ 9. Escalabilidade e Paralelismo

- Utilizar `ExecutorService` (Java) ou `coroutines + Dispatcher` (Kotlin) com pool configur√°vel.
- Usar concorr√™ncia controlada.
- Manter logs por thread ou tarefa.

---

### ‚úÖ 10. Testabilidade

- Todo c√≥digo deve ser test√°vel.
- Usar mocks para depend√™ncias externas.
- Separar scraping da exposi√ß√£o REST.
- Utilizar JUnit (5) e Mockito (ou MockK para Kotlin) para testes unit√°rios.

---

### ‚úÖ 11. Princ√≠pios SOLID

#### S ‚Äì Single Responsibility Principle (SRP)
- Cada classe deve ter uma √∫nica responsabilidade.
- Um componente deve ter apenas uma raz√£o para mudar.

#### O ‚Äì Open/Closed Principle (OCP)
- O c√≥digo deve ser aberto para extens√£o, fechado para modifica√ß√£o.
- Usar abstra√ß√µes para permitir novas funcionalidades sem alterar o core.

#### L ‚Äì Liskov Substitution Principle (LSP)
- Subclasses e implementa√ß√µes devem manter o comportamento esperado das abstra√ß√µes.
- Nenhuma implementa√ß√£o deve quebrar a substituibilidade.

#### I ‚Äì Interface Segregation Principle (ISP)
- Interfaces devem ser pequenas e coesas.
- Nenhuma implementa√ß√£o deve depender de m√©todos que n√£o usa.

#### D ‚Äì Dependency Inversion Principle (DIP)
- Sempre depender de interfaces, n√£o de implementa√ß√µes concretas.
- As depend√™ncias devem ser injetadas via construtor.

---

### ‚úÖ 12. Boas Pr√°ticas com Kotlin

- Utilizar `extension functions` para reutilizar l√≥gica comum.
- Preferir `data classes` para modelar entidades imut√°veis.
- Usar `sealed classes` para representar estados e falhas de scraping.
- Utilizar `destructuring declarations` quando apropriado.
- Evitar `!!` (non-null assertions); preferir `?.`, `let`, `run`, `also`, `takeIf`.
- Evitar `companion object` excessivo; preferir inje√ß√£o de depend√™ncia.

---

### ‚úÖ 13. Observabilidade e Monitoramento

- Adicionar m√©tricas personalizadas (ex: Micrometer, Prometheus).
- Criar painel de observabilidade com status por dom√≠nio ou tarefa.
- Emitir eventos de scraping via log estruturado ou filas (ex: Kafka, RabbitMQ).

---

### ‚úÖ 14. Gerenciamento de Sess√£o e Detec√ß√£o de Bloqueios

- Detectar p√°ginas de bloqueio/CAPTCHA.
- Implementar reconhecimento de padr√£o HTML para identificar mudan√ßas de layout.
- Rotacionar proxies e IPs dinamicamente.
- Criar rotina de teste de sess√£o automatizada.

---

### ‚úÖ 15. Resili√™ncia por Site (Scraping Contextual)

- Criar camada de abstra√ß√£o por dom√≠nio (ex: AmazonScraper, MercadoLivreScraper).
- Permitir overrides de l√≥gica de parsing por dom√≠nio ou categoria.
- Utilizar feature flags para ativar/desativar scrapers individualmente.

---

### ‚úÖ 16. Organiza√ß√£o por M√≥dulo

- Separar o projeto em m√≥dulos independentes, como:
    - `scraper-core`: interfaces e utilit√°rios
    - `scraper-domain`: entidades e l√≥gica de neg√≥cio
    - `scraper-adapters`: implementa√ß√µes Selenium, banco, etc.
    - `scraper-api`: REST, agendamento, autentica√ß√£o

---

Essas regras devem ser seguidas rigorosamente em todo o projeto.

